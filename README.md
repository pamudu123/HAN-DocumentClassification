# Hierarchical Attention Networks (HAN) for Document Classification

> **Paper**: Hierarchical Attention Networks for Document Classification (2016)  
> **Authors**: Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy

**Key Features of this approach:**
- Hierarchical document structure modeling (words → sentences → documents)
- Dual attention mechanisms for words and sentences
- Superior performance on document classification tasks
- Interpretable attention weights for understanding model decisions

## 💡 Key Innovation

> *"This hierarchical approach mirrors human reading comprehension with sentences followed by words and gives attention to important words and sentences. Very creative idea!"*

The HAN architecture addresses the limitation of traditional RNNs in handling long sequences by:
1. **Word-level attention**: Identifying important words within sentences
2. **Sentence-level attention**: Determining which sentences are most relevant for classification
3. **Hierarchical encoding**: Building document representations from word and sentence encodings

## 📚 Resources

### 🎥 Video Presentation
**YouTube**: [HAN Document Classification Presentation](https://www.youtube.com/watch?v=JfgFRSjEucE&t=43s&ab_channel=PamuduRanasinghe)

### 📄 Research Paper
**Original Paper**: [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf)

### 💻 Implementation
**Paper with Code**: [Coming Soon]

---

## 🖼️ Presentation Slides

<details>
<summary>Click to view all slides (15 slides total)</summary>

![Slide 1 - Title](presentation_slides/slide1.PNG)
![Slide 2 - Introduction](presentation_slides/slide2.PNG)
![Slide 3 - Motivation](presentation_slides/slide3.PNG)
![Slide 4 - Problem Statement](presentation_slides/slide4.PNG)
![Slide 5 - Architecture Overview](presentation_slides/slide5.PNG)
![Slide 6 - Word Encoder](presentation_slides/slide6.PNG)
![Slide 7 - Word Attention](presentation_slides/slide7.PNG)
![Slide 8 - Sentence Encoder](presentation_slides/slide8.PNG)
![Slide 9 - Sentence Attention](presentation_slides/slide9.PNG)
![Slide 10 - Document Classification](presentation_slides/slide10.PNG)
![Slide 11 - Experimental Setup](presentation_slides/slide11.PNG)
![Slide 12 - Results](presentation_slides/slide12.PNG)
![Slide 13 - Analysis](presentation_slides/slide13.PNG)
![Slide 14 - Visualization](presentation_slides/slide14.PNG)
![Slide 15 - Conclusion](presentation_slides/slide15.PNG)

</details>

## 📝 Usage & Attribution

You are welcome to use any of the slides, vector images, and SVG image codes for your academic work.  

## 📖 References

1. **Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E.** (2016). *Hierarchical Attention Networks for Document Classification*. In Proceedings of NAACL-HLT. [[PDF]](https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf)
2. **Medentsiy, V.** *Document Representations*. University of Amsterdam. [[Slides]](https://cl-illc.github.io/semantics/resources/slides/DocumentRepresentations.pdf)

---

**📧 Contact**: For questions or collaborations, feel free to reach out through the YouTube channel or GitHub.

**⭐ Star this repository** if you find it helpful for your research or studies!